# -*- coding: utf-8 -*-
"""Modelo2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PHaBTBKeZpExVGmQQYQMiEat_aORSsQS
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.multioutput import MultiOutputRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import classification_report
from sklearn.preprocessing import MinMaxScaler
import joblib

df_parte1 = pd.read_csv("Urban Air Quality and Health Impact Dataset.csv")
df_parte2 = pd.read_csv("accident data.csv")

"""# Modelo 1"""

df_parte1.info()

df_parte1 = df_parte1.drop(
    columns=[
        'description', 'icon', 'stations', 'source', 'conditions',
        'datetimeEpoch', 'sunrise', 'sunriseEpoch',
        'sunset', 'sunsetEpoch', 'precipcover',
        'preciptype', 'snow', 'snowdepth',
        'winddir', 'severerisk',
        'Condition_Code', 'Severity_Score',
        'Health_Risk_Score'
    ],
    errors='ignore'
)

df_parte1.dropna(inplace=True)
df_parte1.drop_duplicates(inplace=True)
df_parte1.isna().sum()

df_parte1['datetime']

df_parte1['datetime'] = pd.to_datetime(df_parte1['datetime'])
df_parte1['date'] = df_parte1['datetime'].dt.date

df_parte1.info()

"""# Modelo 2"""

df_parte2.info()

df_parte2 = df_parte2.drop(columns=[
    'Index',
    'Latitude',
    'Longitude',
    'Light_Conditions',
    'Road_Surface_Conditions',
    'Road_Type',
    'Urban_or_Rural_Area',
    'Weather_Conditions',
    'Vehicle_Type',
    'Number_of_Vehicles',
    'Accident_Severity'
])

df_parte2['Accident Date'] = pd.to_datetime(
    df_parte2['Accident Date'],
    errors='coerce'
)

df_parte2['date'] = df_parte2['Accident Date'].dt.date
df_parte2 = df_parte2.drop(columns=['Accident Date'])
df_parte2 = df_parte2.dropna()

accident_rate = (
    df_parte2
    .groupby(['date', 'District Area'])
    .size()
    .reset_index(name='accident_rate')
)

accident_rate['accident_rate'].describe()



"""# Conexion de datasets"""

accident_rate = accident_rate.rename(
    columns={'District Area': 'City'}
)

df_parte1['date'] = pd.to_datetime(df_parte1['date']).dt.date
accident_rate['date'] = pd.to_datetime(accident_rate['date']).dt.date

print(df_parte1[['date', 'City']].head())
print(accident_rate[['date', 'City']].head())

df_merged = df_parte1.merge(
    accident_rate,
    on=['date', 'City'],
    how='left'
)
df_merged['accident_rate'] = df_merged['accident_rate'].fillna(0)

df_merged.info()
df_merged['accident_rate'].describe()

df_parte1['City'].value_counts()

accident_rate['City'].value_counts()

import numpy as np

df_forced = df_parte1.copy()

# Crear una columna accident_rate aleatoria para pruebas
np.random.seed(42)
df_forced['accident_rate'] = np.random.randint(0, 10, size=len(df_forced))

# Si quieres, también puedes asignar una "City" aleatoria del accident_rate
cities = accident_rate['City'].unique()
df_forced['City'] = np.random.choice(cities, size=len(df_forced))

# Revisar
print(df_forced[['date','City','accident_rate']].head(10))
print(df_forced['accident_rate'].describe())

"""# Entrenamiento del modelo"""

predictors = ['temp', 'tempmax', 'tempmin', 'feelslike', 'feelslikemax', 'feelslikemin']
target = 'accident_rate'

X = df_forced[predictors]
y = df_forced[target]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)

y_pred = model.predict(X_test_scaled)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("MAE:", mae)
print("MSE:", mse)
print("RMSE:", rmse)
print("R2:", r2)

mean_accidents = df_forced.groupby('City')['accident_rate'].mean().sort_values(ascending=False).head(10)

plt.figure(figsize=(10,6))
mean_accidents.plot(kind='bar', color='skyblue')
plt.ylabel("Promedio Accident Rate")
plt.title("Top 10 Ciudades con Mayor Accident Rate (simulado)")
plt.xticks(rotation=45)
plt.show()

daily_accidents = df_forced.groupby('date')['accident_rate'].mean()

plt.figure(figsize=(12,6))
daily_accidents.plot(kind='line', color='orange')
plt.xlabel("Fecha")
plt.ylabel("Accident Rate Promedio")
plt.title("Accident Rate a lo Largo del Tiempo")
plt.grid(True)
plt.show()

bins = [df_forced['accident_rate'].min()-1, 3, 6, df_forced['accident_rate'].max()+1]
y_test_cat = pd.cut(y_test, bins=bins, labels=["Bajo","Medio","Alto"])
y_pred_cat = pd.cut(y_pred, bins=bins, labels=["Bajo","Medio","Alto"])

cm = confusion_matrix(y_test_cat, y_pred_cat, labels=["Bajo","Medio","Alto"])
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Bajo","Medio","Alto"])
disp.plot(cmap=plt.cm.Blues)
plt.title("Matriz de Confusión (Accident Rate categorizado)")
plt.show()